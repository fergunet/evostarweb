<?php include('header.php') ?>

        <h2>Evo<span class="red">MUSART</span><h2>
   </div> 
 <h2>4th International Conference on Evolutionary and Biologically Inspired 
      Music, Sound, Art and Design</h2>
    <p>April 2015, Copenhagen, Denmark<br />
      Part of evo<span class="red">*</span> 2015<br />
      evo<span class="red">*</span>: <a href="http://www.evostar.org">http://www.evostar.org</a></p>
    
          <ul>
<li><a href="#abstracts">Accepted paper abstracts</a></li>
<li><a href="#best">Best Paper Nominees</a></li> 
<li><a href="pdfs/EvoMUSART2015sessions.pdf">Download EuroCOP programme</a></li>
(Note: programmes are provisional and subject to change until the final version is posted).
</ul>


    <hr/>
<strong>NEW THIS YEAR: LEONARDO Gallery</strong>
    <p>
      <br />
     The journal LEONARDO will be publishing a Gallery Section (online and in the 
print edition) associated with the conference. This will consist of a number 
of visual artworks based on ideas and techniques presented at the conference. 
A separate call for this will be issued after papers have been selected for 
the conference.
<br/><br/>
<a href="http://www.leonardo.info/gallery/">http://www.leonardo.info/gallery/</a>
<hr/>
    </p>
    <p>Following the success of previous events and the importance of the field 
      of evolutionary and biologically inspired (artificial neural network, swarm, 
      alife) music, sound, art and design, evomusart has become an evo* conference 
      with independent proceedings since 2012. Thus, evomusart 2015 is the fourth International Conference on Evolutionary and 
      Biologically Inspired Music, Sound, Art and Design.</p>
    <p>The use of biologically inspired techniques for the development of artistic 
      systems is a recent, exciting and significant area of research. There is 
      a growing interest in the application of these techniques in fields such 
      as: visual art and music generation, analysis, and interpretation; sound 
      synthesis; architecture; video; poetry; design; and other creative tasks.</p>
    <p>The main goal of evo<span class="red">musart</span> 2015 is to bring together researchers who are 
      using biologically inspired computer techniques for artistic tasks, providing 
      the opportunity to promote, present and discuss ongoing work in the area. 
    </p>
    <p>The event will be held in April, 2015 in Copenhagen, Denmark, as
      part of the evo<span class="red">*</span> event.</p>
    <h2><br />
      Publication Details </h2>
<p>Submissions will be rigorously reviewed for scientific and artistic merit. Accepted papers will be presented orally or as posters at the event and included in the evomusart proceedings, published by Springer Verlag in a dedicated volume of the Lecture Notes in Computer Science series. The acceptance rate at evomusart 2014 was 26.7% for papers accepted for oral presentation, or 36.7% for oral and poster presentation combined. </p>

<p>Submitters are strongly encouraged to provide in all papers a link for download of media demonstrating their results, whether music, images, video, or other media types. Links should be anonymised for double-blind review, e.g. using a URL shortening service.</p>
    <h2>Topics of interest </h2>
    <p>Submissions should concern the use of biologically inspired computer techniques 
      -- e.g. Evolutionary Computation, Artificial Life, Artificial Neural Networks, 
      Swarm Intelligence, other artificial intelligence techniques -- in the generation, 
      analysis and interpretation of art, music, design, architecture and other 
      artistic fields. Topics of interest include, but are not limited to:</p>
    <p><strong> Generation</strong></p>
    <ul>
      <li>Biologically Inspired Design and Art -- Systems that create drawings, 
        images, animations, sculptures, poetry, text, designs, webpages, buildings, 
        etc.;</li>
      <li> Biologically Inspired Sound and Music -- Systems that create musical 
        pieces, sounds, instruments, voices, sound effects, sound analysis, etc.;</li>
      <li> Robotic-Based Evolutionary Art and Music;</li>
      <li> Other related artificial intelligence or generative techniques in the 
        fields of Computer Music, Computer Art, etc.;</li>
    </ul>
    <p><strong>Theory</strong></p>
    <ul>
      <li>Computational Aesthetics, Experimental Aesthetics; Emotional Response, 
        Surprise, Novelty;</li>
      <li>Representation techniques;</li>
      <li> Surveys of the current state-of-the-art in the area; identification 
        of weaknesses and strengths; comparative analysis and classification;</li>
      <li>Validation methodologies;</li>
      <li> Studies on the applicability of these techniques to related areas;</li>
      <li>New models designed to promote the creative potential of biologically 
        inspired computation; </li>
    </ul>
    <p><strong>Computer Aided Creativity and computational creativity</strong></p>
    <ul>
      <li> Systems in which biologically inspired computation is used to promote 
        the creativity of a human user;</li>
      <li> New ways of integrating the user in the evolutionary cycle;</li>
      <li> Analysis and evaluation of: the artistic potential of biologically 
        inspired art and music; the artistic processes inherent to these approaches; 
        the resulting artefacts;</li>
      <li> Collaborative distributed artificial art environments;</li>
    </ul>
    <p><strong>Automation</strong></p>
    <ul>
      <li>Techniques for automatic fitness assignment</li>
      <li> Systems in which an analysis or interpretation of the artworks is used 
        in conjunction with biologically inspired techniques to produce novel 
        objects;</li>
      <li> Systems that resort to biologically inspired computation to perform 
        the analysis of image, music, sound, sculpture, or some other types of 
        artistic object. </li>
    </ul>
    <!--<p><strong>Important Dates</strong></p>
    <p>Submission deadline: <del>1 November 2013</del><strong> 11 November 2013</strong><br />
      Notification: 06 January 2014<br />
      Camera ready: 01 February 2014<br />
      <br />
      Evo<span class="red">*</span>: 23-25 April 2015</p>-->
    <p><strong>Additional information and submission details</strong></p>
    <p>Submit your manuscript, at most 12 A4 pages long, in Springer LNCS format 
      (instructions downloadable from <a href="http://www.springer.com/computer/lncs?SGWID=0-164-6-793341-0">http://www.springer.com/computer/lncs?SGWID=0-164-6-793341-0</a>) 
      no later than November 15th, 2014.</p>
    <p>Submission link: <a href="http://myreview.csregistry.org/evomusart15">http://myreview.csregistry.org/evomusart15/</a><br />
      page limit: 12 pages</p>
    <p>The reviewing process will be double-blind; please omit information about 
      the authors in the submitted paper.</p>
    <h2>Programme committee</h2>
    <ul>

<li>Alain Lioret, Paris 8 University, France</li>
<li>Alan Dorin, Monash University, Australia</li>
<li>Alejandro Pazos, University of A Coru&ntilde;a, Spain</li>
<li>Amilcar Cardoso, University of Coimbra, Portugal</li>
<li>Amy K. Hoover, University of Central Florida, USA</li>
<li>Andrew Brown, Griffith University, Australia</li>
<li>Andrew Gildfind, Google, Inc., Australia</li>
<li>Andrew Horner, University of Science and Technology, Hong Kong</li>
<li>Anna Ursyn, University of Northern Colorado, USA</li>
<li>Antonino Santos, University of A Coru&ntilde;a, Spain</li>
<li>Antonios Liapis, IT University of Copenhagen , Denmark</li>
<li>Arne Eigenfeldt, Simon Fraser University, Canada</li>
<li>Benjamin Smith, Indianapolis University, Purdue University,Indianapolis, USA</li>
<li>Bill Manaris, College of Charleston, USA</li>
<li>Brian Ross, Brock University, Canada</li>
<li>Carlos Grilo, Instituto Politecnico de Leiria, Portugal</li>
<li>Christian Jacob, University of Calgary, Canada</li>
<li>Dan Ashlock, University of Guelph, Canada</li>
<li>Dan Ventura, Brigham Young University, USA</li>
<li>Daniel Jones, Goldsmiths College,  University of London, UK</li>
<li>Daniel Silva, University of Coimbra, Portugal</li>
<li>Douglas Repetto, Columbia University, USA</li>
<li>Eduardo Miranda, University of Plymouth, UK</li>
<li>Eleonora Bilotta , University of Calabria, Italy</li>
<li>Gary Greenfield, University of Richmond, USA</li>
<li>Hans Dehlinger, Independent Artist, Germany</li>
<li>Jane Prophet, City University, Hong Kong, China</li>
<li>Jon McCormack, Monash University, Australia</li>
<li>Jonathan Byrne, University College Dublin, Ireland</li>
<li>Jonathan E. Rowe, University of Birmingham, UK</li>
<li>Jonathan Eisenmann, Ohio State University, USA</li>
<li>Jose Fornari, NICS/Unicamp, Brazil</li>
<li>Juan Romero, University of A Coru&ntilde;a, Spain</li>
<li>Kate Reed, Imperial College, UK</li>
<li>Marcelo Freitas Caetano, IRCAM, France</li>
<li>Marcos Nadal, University of Vienna, Austria</li>
<li>Matthew Lewis, Ohio State University, USA</li>
<li>Michael O'Neill, University College Dublin, Ireland</li>
<li>Nicolas Monmarch, University of Tours, France</li>
<li>Palle Dahlstedt, Göteborg University, Sweden</li>
<li>Patrick Janssen , National University of Singapure, Singapure</li>
<li>Paulo Urbano, Universidade de Lisboa, Portugal</li>
<li>Pedro Abreu, University of Coimbra, Portugal</li>
<li>Pedro Cruz, University of Coimbra, Portugal</li>
<li>Penousal Machado, University of Coimbra, Portugal</li>
<li>Peter Bentley, University College London , UK</li>
<li>Peter Cariani, University of Binghamton, USA</li>
<li>Philip Galanter, Texas A&amp;M College of Architecture, USA</li>
<li>Philippe Pasquier, Simon Fraser University, Canada</li>
<li>Roger Malina, International Society for the Arts, Sciences and Technology, USA</li>
<li>Roisin Loughran, University College Dublin, Ireland</li>
<li>Ruli Manurung, University of Indonesia, Indonesia</li>
<li>Scott Draves, Independent Artist, USA</li>
<li>Somnuk Phon-Amnuaisuk, Brunei Institute of Technology, Malaysia</li>
<li>Stephen Todd, IBM, UK</li>
<li>Takashi Ikegami, Tokyo Institute of Technology, Japan</li>
<li>Tim Blackwell, Goldsmiths College,  University of London, UK</li>
<li>Hern&aacute;n Kerlle&ntilde;evich, National University of Quilmes, Argentina</li>
<li>Yang Li, University of Science and Technology Beijing, China</li>

    </ul>
    <h2>Evo<span class="red">MUSART</span> Conference chairs </h2>
    <p><strong>Colin Johnson</strong><br />
       University of Kent, UK<br />
      <em>c.g.johnson@kent.ac.uk</em></p>
    <p><strong>Adrián Carballal</strong><br />
      University of A Coru&ntilde;a, Spain<br />
      <em>adriancarballal@gmail.com</em></p>
    <h2>Evo<span class="red">MUSART </span>Publication chair</h2>
    <p><strong>Jo&atilde;o Correia</strong><br />
      University of Coimbra<br />
      <em>jncor(at)dei.uc.pt</em><br />
    </p>


<a name="abstracts"></a>
<hr/>
<h3>Accepted paper abstracts</h3>
<ul>

<li><strong>Avoidance Drawings Evolved Using Virtual Drawing Robots</strong></li>
<em>Gary Greenfield</em><br/>
We introduce a generative system for "avoidance drawings", drawings made by virtual drawing robots executing a random walk while simultaneously avoiding the paths of other robots. The random walk method is unique and is based on a curvature controlling
scheme initially introduced by Chappell. We design a fitness function for evaluating avoidance drawings and an evolutionary framework for evolving them. This requires us to follow principles we elucidate for simulated evolution where the generative system is highly stochastic in nature. Examples document the evolutionary system's efficacy and success.


<li><strong>Toward Certain Sonic Properties of an Audio Feedback System by Evolutionary Control of Second-Order Structures</strong></li>
<em>Seunghun Kim, Juhan Nam, Graham Wakefield</em><br/>
Aiming for high-level intentional control of audio feedback, though microphones, loudspeakers and digital signal processing, we present a system adapting toward chosen sonic features. Users control the system by selecting and changing feature objectives in real-time. The system has a second-order structure in which the internal signal processing algorithms are developed according to an evolutionary process. Genotypes develop into signal-processing algorithms, and fitness is measured by analysis of the incoming audio feedback. A prototype is evaluated experimentally to measure changes of audio feedback depending on the chosen target conditions. By enhancing interactivity of an audio feedback through the intentional control, we expect that feedback systems could be utilized more effectively in the fields of musical interaction, finding balance between nonlinearity and interactivity.


<li><strong>Generative Music with Stochastic Diffusion Search</strong></li>
<em>Asmaa Majid al-Rifaie, Mohammad Majid al-Rifaie</em><br/>
This paper introduces an approach for using a swarm intelligence algorithm, Stochastic Diffusion Search (SDS) - inspired by one species of ants, Leptothorax acervorum - in order to generate music from plain text. In this approach, SDS is adapted in such a way to vocalise the agents, to hear their "chit-chat". While the generated music depends on the input text, the algorithm's search capability in locating the words in the input text is reflected in the duration and dynamic of the resulting musical notes. In other words, the generated music depends on the behaviour of the algorithm and the communication between its agents. This novel approach, while staying loyal to the original input text, when run each time, "vocalises" the input text in varying "flavours".


<li><strong>Evolving Diverse Design Populations using Fitness Sharing and Random Forest based Fitness Approximation</strong></li>
<em>Kate Reed, Duncan Gillies</em><br/>
A large, diverse design space will contain many non-viable designs. To locate the viable designs we need to have a method of testing the designs and a way to navigate the space. We have shown that using machine learning on artificial data can accurately predict the viability of chairs based on a range of ergonomic considerations. We have also shown that the design space can be explored using an evolutionary algorithm with the predicted viability as a fitness function. We find that this method in conjunction with a fitness sharing technique can maintain a diverse population with many potential viable designs.


<li><strong>Interior Illumination Design Using Genetic Programming</strong></li>
<em>Kelly Moylan, Brian Ross</em><br/>
Interior illumination is a complex problem involving numerous interacting factors. This research applies genetic programming towards problems in illumination design. The Radiance system is used for performing accurate illumination simulations. Radiance accounts for a number of important environmental factors, which we exploit during fitness evaluation. Illumination requirements include local illumination intensity from natural and artificial sources, colour, and uniformity. Evolved solutions incorporate design elements such as artificial lights, room materials, windows, and glass properties. A number of case studies are examined, including a many-objective problem involving 6 illumination requirements, the design of a decorative wall of lights, and the creation of a stained-glass window for a large public space. Our results show the technical and creative possibilities of applying genetic programming to illumination design.


<li><strong>The Sound Digestive System: a strategy for music and sound composition</strong></li>
<em>Juan Manuel Escalante</em><br/>
Sound Digestive System is an audio visual project that uses the digestive system processes into algorithmic sound composition. This project proposes different strategies to bring bio-data, translations and interpretations of living processes into the sound domain, thus generating an artistic result based on scientific data. To hear the result and browse additional material visit the following address: http://goo.gl/DT1Wy0


<li><strong>AudioInSpace: A Proof-of-Concept Exploring the Creative Fusion of Generative Audio, Visuals and Gameplay</strong></li>
<em>Amy K. Hoover, William Cachia, Antonios Liapis, Georgios N. Yannakakis</em><br/>
Computer games are unique creativity domains in that they elegantly fuse several facets of creative work including visuals, narrative, music, architecture and design. While the exploration of possibilities across facets of creativity offers a more realistic approach to the game design process, most existing autonomous (or semi-autonomous) game content generators focus on the mere generation of single domains (creativity facets) in games. Motivated by the sparse literature on multifaceted game content generation, this paper introduces a multifaceted procedural content generation (PCG) approach that is based on the interactive evolution of multiple artificial neural networks that orchestrate the generation of visuals, audio and gameplay. The approach is evaluated on a spaceship shooter game. The generated artefacts - a fusion of audiovisual and gameplay elements - showcase the capacity of multifaceted PCG and its evident potential for computational game creativity.



<li><strong>Music with Unconventional Computing: Towards a Step Sequencer From Plasmodium of Physarum Polycephalum</strong></li>
<em>Edward Braund, Eduardo Miranda</em><br/>
The field of computer music has evolved in tandem with advances made in computer science. We are interested in how the developing field of unconventional computation may provide new pathways for music and related technologies. In this paper, we outline our initial work into harnessing the behaviour of the biological computing substrate Physarum polycephalum for a musical step sequencer. The plasmodium of Physarum polycephalum is an amorphous unicellular organism, which moves like a giant amoeba as it navigates its environment for food. Our research manipulates the organism's route-efficient propagation characteristics in order to create a growth environment for musical/sound arrangement. We experiment with this device in two different scenarios: sample triggering and MIDI note triggering using sonification techniques.


<li><strong>Towards an Evolutionary Computational Approach to Articulatory Vocal Synthesis with PRAAT</strong></li>
<em>Jared Drayton, Eduardo Miranda</em><br/>
This paper presents our current work into developing an evolutionary computing approach to articulatory speech synthesis. Specifically, we implement genetic algorithms to find optimised parameter combinations for the re-synthesis of a vowel using the articulatory synthesiser PRAAT. Our framework analyses the target sound using Fast Fourier Transform (FFT) to obtain formant information, which is then harnessed in a fitness function applied to a real valued genetic algorithm using a generation size of 75 sounds over 50 generations. In this paper, we present three differently configured genetic algorithms (GAs) and offer a comparison of their suitability for elevating the average fitness of the re-synthesised sounds.


<li><strong>Moody Music Generator: Characterising Control Parameters Using Crowdsourcing</strong></li>
<em>Marco Scirea, Julian Togelius, Mark Nelson</em><br/>
We characterise the expressive effects of a music generator capable of varying its moods through two control parameters. The two control parameters were constructed on the basis of existing work on valence and arousal in music, and intended to provide control over those two mood factors. In this paper we conduct a listener study to determine how people actually perceive the various moods the generator can produce. Rather than directly attempting to validate that our two control parameters represent arousal and valence, instead we conduct an open-ended study to crowd-source labels characterising different parts of this two-dimensional control space. Our aim is to characterise perception of the generator's expressive space, without constraining listeners' responses to labels specifically aimed at validating the original arousal/valence motivation. Subjects were asked to listen to clips of generated music over the Internet, and to describe the moods with free-text labels. We find that the arousal parameter does roughly map to perceived arousal, but that the nominal "valence" parameter has strong interaction with the arousal parameter, and produces different effects in different parts of the control space. We believe that the characterisation methodology described here is general and could be used to map the expressive range of other parameterisable generators.


<li><strong>Evotype: Evolutionary Type Design</strong></li>
<em>Tiago Martins, João Correia, Ernesto Costa, Penousal Machado</em><br/>
An evolutionary generative system for type design, Evotype, is described. The system uses a Genetic Algorithm to evolve a set of individuals composed of line segments, each encoding the shape of a specific character, i.e. a glyph. To simultaneously evolve glyphs for the entire alphabet, an island model is adopted. To assign fitness we resort to a scheme based on Optical Character Recognition. We study the evolvability of the proposed approach as well as the impact of the migration in the evolutionary process. The migration mechanism is explored through three experimental setups: fitness guided migration, random migration, and no migration. We analyse the experimental results in terms of fitness, migration paths, and appearance of the glyphs. The results show the ability of the system to find suitable glyphs and the impact of the migration strategy in the evolutionary process.


<li><strong>Lichtsuchende: exploring the emergence of a cybernetic society</strong></li>
<em>Dave Murray-Rust, Rocio von Jungenfeld</em><br/>
In this paper, we describe "Lichtsuchende", an interactive installation, built using a society of biologically inspired, cybernetic creatures who exchange light as a source of energy and a means of communication. Visitors are invited to engage with the installation using torches to influence and interact with the phototropic robots. As well as describing the finished piece, we explore some of the issues around creating works based on biologically inspired robots. We present an account of the development of the creatures in order to highlight the gulfs between conceptual ideas of how to allow emergent behaviours and the manners in which they are implemented. We also expose the interrelations and tensions between the needs of the creatures as they emerge and the needs of the creators, to understand the duet between the cyber-organisms and their initiators. Finally, we look at the ways in which creators, robots and visitors are enrolled, to perform their functions, so that the network of activity can be woven between all parties.


<h3>POSTERS</h3>

<li><strong>Biological Content Generation: Evolving Game Terrains through Living Organisms</strong></li>
<em>Wim van Eck, Maarten Lamers</em><br/>
This study explores the concept of evolving game terrains through intermediation of living biological organisms and presents a proof of concept realization thereof. We explore how fungal and bacterial cultures can be used to generate an evolving game terrain in real-time. By visually capturing growing cultures inside a Petri-dish, heightmaps are generated that form the basis of naturally evolving terrains. Possible consequences and benefits of this approach are discussed, as are its effects on the visual appearance of simulated terrains. A novel and convenient method for visually capturing growing microorganisms is presented, with a technical description for translating captured footage to virtual terrains. This work is experimental in nature and is an initial venture into the novel domain of organically growing virtual terrains.


<li><strong>DrawCompileEvolve: Sparking Interactive Evolutionary Art with Human Creations</strong></li>
<em>Jinhong Zhang, Rasmus Taarnby, Antonios Liapis, Sebastian Risi</em><br/>
This paper presents DrawCompileEvolve, a web-based drawing tool which allows users to draw simple primitive shapes, group them together or define patterns in their groupings (e.g. symmetry, repetition). The user's vector drawing is then compiled into an indirectly encoded genetic representation, which can be evolved interactively, allowing the user to change the image's colors, patterns and ultimately transform it. The human artist has direct control while drawing the initial seed of an evolutionary run and indirect control while interactively evolving it, thus making DrawCompileEvolve a mixed-initiative art tool. Early results in this paper show the potential of DrawCompileEvolve to jump-start evolutionary art with meaningful drawings as well as the power of the underlying genetic representation to transform the user's initial drawing into a different, yet potentially meaningful, artistic rendering.


<li><strong>On the Stylistic Evolution of a Society of Virtual Melody Composers</strong></li>
<em>Valerio Velardo, Mauro Vallati</em><br/>
In the field of computational creativity, the area of automatic music generation deals with techniques that are able to automatically compose human-enjoyable music. Although investigations in the area started recently, numerous techniques based on artificial intelligence have been proposed. Some of them produce pleasant results, but none is able to evolve the style of the musical pieces generated. In this paper, we fill this gap by proposing an evolutionary memetic system that composes melodies, exploiting a society of virtual composers. An extensive validation, performed by using both quantitative and qualitative analyses, confirms that the system is able to evolve its compositional style over time.



<li><strong>Feature Discovery by Deep Learning for Aesthetic Analysis of Evolved Abstract Images</strong></li>
<em>Allan Campbell, Vic Ciesielski, A. K. Qin</em><br/>
We investigated the ability of a Deep Belief Network with logistic nodes, trained unsupervised by Contrastive Divergence, to discover features of evolved abstract art images. Two Restricted Boltzmann Machine models were trained independently on low and high aesthetic class images. The receptive fields (filters) of both models were compared by visual inspection. Roughly 10% of these filters in the high aesthetic model approximated the form of the high aesthetic training images. The remaining 90% of filters in the high aesthetic model and all filters in the low aesthetic model appeared noise like. The form of discovered filters was not consistent with the Gabor filter like forms discovered for MNIST training data, possibly revealing an interesting property of the evolved abstract training images.We joined the datasets and trained a Restricted Boltzmann Machine finding that roughly 30% of the filters approximate the form of the high aesthetic input images. We trained a 10 layer Deep Belief Network on the joint dataset and used the output activities at each layer as training data for traditional classifiers (decision tree and random forest). The highest classification accuracy from learned features (84%) was achieved at the second hidden layer, indicating that the features discovered by our Deep Learning approach have discriminative power. Above the second hidden layer, classification accuracy decreases.



<li><strong>Automatic Generation of Chord Progressions with an Arti cial Immune System</strong></li>
<em>Maria Navarro, Leandro Nunes de Castro, Marcelo Caetano, Gilberto Bernandes, Juan Manuel Corchado</em><br/>
Chord progressions are widely used in music. The automatic generation of chord progressions can be challenging because it depends on many factors, such as the musical context. This work proposes a penalty function that encodes musical rules to automatically generate chord progressions. Then we use an arti cial immune system (AIS) to minimize the penalty function when proposing candidates for the next chord in a sequence. The AIS is capable of nding multiple optima in parallel, resulting in several di_erent chords as appropriate candidates. Thus we performed a listening test to evaluate the chords subjectively and validate the penalty function. We found that chords with a low penalty value were considered better candidates than chords with higher penalty values.


<li><strong>Schemographe: Application for a New Representation Technique and Methodology of Analysis in Tonal Harmony</strong></li>
<em>Anna Shvets, Myriam Desainte-Cathérine</em><br/>
A recent development of music theory focuses basically on neo-riemannian angle of harmonic analysis with the use of Tonnetz as a space for harmonic change representation. However the Tonnetz does not cover the functional relations between accords within tonality and is feebly suitable to capture the features of neo-tonal postmodern music based on a new use of tonal functionality. This work presents an alternative method for music harmony progressions representation and analysis which uses two levels of representation. The first level is represented as a system of horizontal and vertical triads of graphs where each graph is an exo-frame filled out by information of specified degree of the scale. The graph pattern in this system represents the specified segment of harmonic progression taken from harmonic analysis of the musical composition. The pattern is then schematized for the second level of representation which examines its structural resemblance to the other schemas received similarly from the segments of harmonic progression. In order to facilitate the understanding of a new methodology and encourage its use in tonal harmony analysis an Android application for tablets called Schemographe has been created. The application presents the possibilities of the system on the two described levels of representation on example of three vocal pieces by neo-tonal postmodern composer Valentin Silvestrov. Link to the media file: https://drive.google.com/file/d/0BwEd8DFYhDSGUm5mOTR2bG9rb2s/view?usp=sharing



<li><strong>A Genetic Programming Approach to Generating Musical Compositions</strong></li>
<em>David M. Hofmann</em><br/>
Evolutionary algorithms have frequently been applied in the field of computer-generated art. In this paper, a novel approach in the domain of automated music composition is proposed. It is inspired by genetic programming and uses a tree-based domain model of compositions. The model represents musical pieces as a set of constraints changing over time, forming musical contexts allowing to compose, reuse and reshape musical fragments. The system implements a multi-objective optimization aiming for statistical measures and structural features of evolved models. Furthermore a correspondent domain-specific computer language is introduced used to transform domain models to a comprehensive, human-readable text representation and vice versa. The language is also suitable to limit the search space of the evolution and as a composition language for human composers.



<li><strong>Echo</strong></li>
<em>Christoph Klemmt, Rajat Sodhi </em><br/>
This paper is interested in the artistic possibilities of systematic translations of sound or music into three-dimensional form. The generation of static two and three-dimensional form based on music or sound has been used by various artists, architects, scientists and technicians.  The time-based attributes of a sound can be directly transformed into spatial dimensions of the generated form. A two-dimensional example is the visualisation of a sound wave in which the time of the sound is recorded from left to right, while the frequency, another time-based attribute of the sound, is recorded in the vertical direction.  Many attempts in generating systematic three-dimensional translations are taking the form of single-surface morphologies, due to most data which can be extracted from a sound being dependent variables for any given time-frequency coordinate. We are proposing a system of analysing reassigned sound data within a variable time frame as a tool to extract multiple consecutive layers of information, which in their combination have the potential to form non-surface morphologies.  The artistic possibilities of the morphologies as an architectural geometry has been tested with the design of an exhibition for Design Shanghai 2013.


<li><strong>Interpretability of Music Classification as a Criterion for Evolutionary Multi-Objective Feature Selection</strong></li>
<em>Igor Vatolkin, Guenter Rudolph, Claus Weihs</em><br/>
The development of numerous audio signal characteristics led to an increase of classification performance for automatic categorisation of music audio recordings. Unfortunately, models built with such low-level descriptors lack of interpretability. Musicologists and listeners can not learn musically meaningful properties of genres, styles, composers, or personal preferences. On the other side, there are new algorithms for the mining of interpretable features from music data: instruments, moods and melodic properties, tags and meta data from the social web, etc. In this paper, we propose an approach how evolutionary multi-objective feature selection can be applied for a systematic maximisation of interpretability without a limitation to the usage of only interpretable features. We introduce a simple hypervolume based measure for the evaluation of trade-off between classification performance and interpretability and discuss how the results of our study may help to search for particularly relevant high-level descriptors in future.



<li><strong>Chorale Music Splicing System: an algorithmic music composer inspired by molecular splicing</strong></li>
<em>Clelia De Felice, Roberto De Prisco, Delfina Malandrino, Gianluca Zaccagnino, Rocco Zaccagnino, Rosalba Zizza</em><br/>
Splicing systems are a formal model of a generative mechanism of words (strings of characters), inspired by a recombinant behavior of DNA. They are defined by a finite alphabet A, an initial set I of words and a set R of rules. Many of the studies about splicing systems focused on the properties of the generated languages and their theoretical computational power. In this paper we propose the use of splicing systems for algorithmic music composition. Although the approach is general and can be applied to many types of music, in this paper, we focus the attention to the algorithmic composition of 4-voice chorale-like music. We have developed a Java implementation of this approach and we have provided an evaluation of the music output by the system.


<li><strong>FuXi: a Fish-Driven Instrument for Real-Time Music Performance</strong></li>
<em>Joao Cordeiro</em><br/>
In this paper we present a system for real-time computer music performance (live electronics) and live visuals based on the behavior of a fish in an aquarium. The system is comprised of 1) an aquarium with a fish; 2) a computer vision module (USB Camera); 3) a visual display of the fish overlaid by graphical elements controlled by the user, 4) a sound synthesis module and 5) a standard MIDI controller. The musical expression and graphic generation is a combination of the fish movements and decisions made by the performer in real-time, together contributing to the audiovisual experience. By making use of a live animal, the system provides indeterminacy and natural gestures to the sound being generated. The match between sound and image shows some semantic redundancy, aiming at a more narrative compositional approach where the fish is the main character. The system is targeted to soundscape composition and electroacoustic music featuring a high degree of improvisation.

</ul>
<a name="best"></a>
<br/>
<hr>
<h2>Best Paper Candidates</h2>
<ul>
<li><strong>Interior Illumination Design Using Genetic Programming</strong></li>
Kelly Moylan, Brian Ross

 <li><strong>Moody Music Generator: Characterising Control Parameters Using Crowdsourcing</strong></li>

<em>Marco Scirea, Julian Togelius, Mark Nelson</em>

 

<li><strong>Evotype: Evolutionary Type Design</strong></li>

<em>Tiago Martins, João Correia, Ernesto Costa, Penousal Machado</em>

 

<li><strong>Lichtsuchende: Exploring the Emergence of a Cybernetic Society</strong></li>
<em>Dave Murray-Rust, Rocio von Jungenfeld</em>
</ul>

        <?php include('footer.php') ?>
